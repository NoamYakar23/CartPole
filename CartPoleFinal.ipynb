{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "esG2hlYaRoPW"
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import gym # pull the environment\n",
    "\n",
    "import time # to get the time\n",
    "\n",
    "import math # needed for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NGVW6kr5R8U4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U03x-RFgR9ay"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "'''\n",
    "\n",
    "# Learning Rate: learning rate is associated with how big you take a leap\n",
    "lr = 0.1\n",
    "\n",
    "#Discount Factor\n",
    "gamma = 0.95\n",
    "\n",
    "#Amount of iterations we are going to run until we see our model is trained\n",
    "epochs = 60000\n",
    "total_time = 0\n",
    "total_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "step_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "# epsilon is associated with how random you take an action.\n",
    "epsilon = 1\n",
    "\n",
    "#exploration is decaying and we will get to a state of full exploitation\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3Oe4eNvbSEgc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.74722875 0.28324679]\n",
      "  [0.9398173  0.76022376]\n",
      "  [0.7112307  0.35212502]\n",
      "  ...\n",
      "  [0.02773251 0.1938166 ]\n",
      "  [0.74045475 0.99613517]\n",
      "  [0.46113503 0.98897351]]\n",
      "\n",
      " [[0.44710506 0.72808767]\n",
      "  [0.18018294 0.53075297]\n",
      "  [0.49564318 0.43850144]\n",
      "  ...\n",
      "  [0.53332124 0.98937067]\n",
      "  [0.34462578 0.6719122 ]\n",
      "  [0.25566221 0.46102021]]\n",
      "\n",
      " [[0.40732536 0.07767555]\n",
      "  [0.76168508 0.08206577]\n",
      "  [0.60961839 0.48799963]\n",
      "  ...\n",
      "  [0.51073226 0.00168121]\n",
      "  [0.45780774 0.46315303]\n",
      "  [0.61326796 0.31180428]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.70617826 0.20683258]\n",
      "  [0.65451101 0.50292549]\n",
      "  [0.52050943 0.54322778]\n",
      "  ...\n",
      "  [0.03860294 0.64955119]\n",
      "  [0.79204953 0.24459593]\n",
      "  [0.262935   0.91095703]]\n",
      "\n",
      " [[0.81165652 0.09784694]\n",
      "  [0.2474812  0.94389301]\n",
      "  [0.78449587 0.66316326]\n",
      "  ...\n",
      "  [0.34294385 0.23249348]\n",
      "  [0.19998348 0.81215252]\n",
      "  [0.2534052  0.45525725]]\n",
      "\n",
      " [[0.74183806 0.6895331 ]\n",
      "  [0.35723788 0.5072197 ]\n",
      "  [0.28974844 0.70503198]\n",
      "  ...\n",
      "  [0.60410199 0.28402171]\n",
      "  [0.8938225  0.28108372]\n",
      "  [0.53567595 0.78731454]]]\n"
     ]
    }
   ],
   "source": [
    "#randomly initializing values in our q table our q table\n",
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape\n",
    "print(q_table[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Kdg-uq7dSGJ9"
   },
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = state/step_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nk7FcazMSMhu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56543/1130559033.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Average: 0.0008616750240325928\n",
      "Mean Reward: 0.012\n",
      "Episode: 1000\n",
      "Time Average: 0.000532735824584961\n",
      "Mean Reward: 21.414\n",
      "Episode: 2000\n",
      "Time Average: 0.001082008123397827\n",
      "Mean Reward: 21.761\n",
      "Episode: 3000\n",
      "Time Average: 0.0005687038898468018\n",
      "Mean Reward: 21.868\n",
      "Episode: 4000\n",
      "Time Average: 0.0009302968978881836\n",
      "Mean Reward: 22.332\n",
      "Episode: 5000\n",
      "Time Average: 0.000547583818435669\n",
      "Mean Reward: 22.638\n",
      "Episode: 6000\n",
      "Time Average: 0.0011672506332397461\n",
      "Mean Reward: 22.097\n",
      "Episode: 7000\n",
      "Time Average: 0.0005508220195770264\n",
      "Mean Reward: 22.747\n",
      "Episode: 8000\n",
      "Time Average: 0.0008356878757476806\n",
      "Mean Reward: 22.38\n",
      "Episode: 9000\n",
      "Time Average: 0.0005490899085998536\n",
      "Mean Reward: 22.145\n",
      "Episode: 10000\n",
      "Time Average: 0.0006819193363189698\n",
      "Mean Reward: 22.907\n",
      "Episode: 11000\n",
      "Time Average: 0.000538360595703125\n",
      "Mean Reward: 21.921\n",
      "Episode: 12000\n",
      "Time Average: 0.0008269457817077637\n",
      "Mean Reward: 23.412\n",
      "Epsilon: 0.8824941446941661\n",
      "Episode: 13000\n",
      "Epsilon: 0.8607047486686201\n",
      "Time Average: 0.0006817357540130616\n",
      "Mean Reward: 26.129\n",
      "Epsilon: 0.8394533480303666\n",
      "Episode: 14000\n",
      "Time Average: 0.0010064618587493897\n",
      "Mean Reward: 27.084\n",
      "Episode: 15000\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.0007404661178588867\n",
      "Mean Reward: 28.993\n",
      "Episode: 16000\n",
      "Time Average: 0.0010394086837768554\n",
      "Mean Reward: 31.131\n",
      "Epsilon: 0.7225214829355084\n",
      "Episode: 17000\n",
      "Time Average: 0.0008688995838165283\n",
      "Mean Reward: 33.328\n",
      "Epsilon: 0.687282835269431\n",
      "Episode: 18000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.0027893493175506594\n",
      "Mean Reward: 35.152\n",
      "Episode: 19000\n",
      "Epsilon: 0.6376209781063321\n",
      "Time Average: 0.0009578831195831298\n",
      "Mean Reward: 37.398\n",
      "Episode: 20000\n",
      "Time Average: 0.0013562507629394532\n",
      "Mean Reward: 41.643\n",
      "Epsilon: 0.5915475999948323\n",
      "Episode: 21000\n",
      "Time Average: 0.001168140172958374\n",
      "Mean Reward: 45.856\n",
      "Episode: 22000\n",
      "Time Average: 0.0015995962619781495\n",
      "Mean Reward: 48.529\n",
      "Epsilon: 0.5352530648457575\n",
      "Episode: 23000\n",
      "Time Average: 0.00132269024848938\n",
      "Mean Reward: 53.308\n",
      "Epsilon: 0.5091478283790776\n",
      "Episode: 24000\n",
      "Time Average: 0.0018754565715789794\n",
      "Mean Reward: 60.387\n",
      "Episode: 25000\n",
      "Time Average: 0.0015735087394714356\n",
      "Mean Reward: 63.77\n",
      "Epsilon: 0.4606948546521764\n",
      "Episode: 26000\n",
      "Epsilon: 0.44931997732828616\n",
      "Time Average: 0.003104854106903076\n",
      "Mean Reward: 67.39\n",
      "Epsilon: 0.43822595366018774\n",
      "Episode: 27000\n",
      "Time Average: 0.0020104706287384032\n",
      "Mean Reward: 75.268\n",
      "Episode: 28000\n",
      "Epsilon: 0.4065605117212756\n",
      "Time Average: 0.003945989847183228\n",
      "Mean Reward: 78.707\n",
      "Epsilon: 0.396522249086328\n",
      "Episode: 29000\n",
      "Epsilon: 0.3867318381581326\n",
      "Time Average: 0.0021841096878051757\n",
      "Mean Reward: 85.198\n",
      "Episode: 30000\n",
      "Time Average: 0.002125802278518677\n",
      "Mean Reward: 86.671\n",
      "Episode: 31000\n",
      "Time Average: 0.002255427360534668\n",
      "Mean Reward: 94.684\n",
      "Epsilon: 0.3412885827413639\n",
      "Episode: 32000\n",
      "Time Average: 0.002568772315979004\n",
      "Mean Reward: 105.959\n",
      "Epsilon: 0.32464333633178233\n",
      "Episode: 33000\n",
      "Time Average: 0.0025622243881225585\n",
      "Mean Reward: 106.046\n",
      "Epsilon: 0.30880990796138097\n",
      "Episode: 34000\n",
      "Time Average: 0.0027976818084716796\n",
      "Mean Reward: 115.033\n",
      "Epsilon: 0.29374870383187524\n",
      "Episode: 35000\n",
      "Epsilon: 0.28649584342677675\n",
      "Time Average: 0.0030052151679992676\n",
      "Mean Reward: 127.005\n",
      "Epsilon: 0.27942206120438906\n",
      "Episode: 36000\n",
      "Time Average: 0.0032637548446655274\n",
      "Mean Reward: 136.569\n",
      "Episode: 37000\n",
      "Epsilon: 0.25923151114313064\n",
      "Time Average: 0.003137932062149048\n",
      "Mean Reward: 131.937\n",
      "Episode: 38000\n",
      "Epsilon: 0.24658833291124824\n",
      "Time Average: 0.002984999179840088\n",
      "Mean Reward: 124.107\n",
      "Episode: 39000\n",
      "Time Average: 0.0032908966541290285\n",
      "Mean Reward: 139.592\n",
      "Epsilon: 0.22877029070403326\n",
      "Episode: 40000\n",
      "Time Average: 0.003427138566970825\n",
      "Mean Reward: 141.534\n",
      "Epsilon: 0.2176127599440723\n",
      "Episode: 41000\n",
      "Time Average: 0.0036497447490692137\n",
      "Mean Reward: 148.64\n",
      "Episode: 42000\n",
      "Epsilon: 0.20188844202629158\n",
      "Time Average: 0.00356670618057251\n",
      "Mean Reward: 145.774\n",
      "Epsilon: 0.19690367556326213\n",
      "Episode: 43000\n",
      "Epsilon: 0.192041986461381\n",
      "Time Average: 0.004781337022781372\n",
      "Mean Reward: 181.097\n",
      "Episode: 44000\n",
      "Time Average: 0.004117494821548462\n",
      "Mean Reward: 171.638\n",
      "Epsilon: 0.17816536796962992\n",
      "Episode: 45000\n",
      "Time Average: 0.003962650060653687\n",
      "Mean Reward: 158.884\n",
      "Episode: 46000\n",
      "Epsilon: 0.1652914496910655\n",
      "Time Average: 0.004347129821777344\n",
      "Mean Reward: 174.384\n",
      "Epsilon: 0.16121028849740862\n",
      "Episode: 47000\n",
      "Time Average: 0.004401975870132446\n",
      "Mean Reward: 190.373\n",
      "Epsilon: 0.15334777825975254\n",
      "Episode: 48000\n",
      "Time Average: 0.35103354024887085\n",
      "Mean Reward: 175.099\n",
      "Epsilon: 0.14586873652037563\n",
      "Episode: 49000\n",
      "Time Average: 0.005409465789794922\n",
      "Mean Reward: 209.734\n",
      "Epsilon: 0.13875446084395782\n",
      "Episode: 50000\n",
      "Time Average: 0.17228762125968933\n",
      "Mean Reward: 216.879\n",
      "Episode: 51000\n",
      "Epsilon: 0.12872830587316755\n",
      "Time Average: 0.005809013843536377\n",
      "Mean Reward: 189.913\n",
      "Episode: 52000\n",
      "Time Average: 0.005418278932571411\n",
      "Mean Reward: 212.591\n",
      "Epsilon: 0.11942662334734862\n",
      "Episode: 53000\n",
      "Epsilon: 0.11647789670960881\n",
      "Time Average: 0.005204017162322998\n",
      "Mean Reward: 208.309\n",
      "Epsilon: 0.11360197618947\n",
      "Episode: 54000\n",
      "Epsilon: 0.11079706415310193\n",
      "Time Average: 0.008471179246902466\n",
      "Mean Reward: 199.849\n",
      "Episode: 55000\n",
      "Time Average: 0.006206542253494263\n",
      "Mean Reward: 218.822\n",
      "Epsilon: 0.10279106183252165\n",
      "Episode: 56000\n",
      "Time Average: 0.007772777557373047\n",
      "Mean Reward: 240.314\n",
      "Epsilon: 0.09777776036441635\n",
      "Episode: 57000\n",
      "Time Average: 0.006634435176849365\n",
      "Mean Reward: 248.801\n",
      "Epsilon: 0.09300896645525675\n",
      "Episode: 58000\n",
      "Time Average: 0.006497172117233277\n",
      "Mean Reward: 248.523\n",
      "Episode: 59000\n",
      "Time Average: 0.0061646454334259035\n",
      "Mean Reward: 243.011\n",
      "Epsilon: 0.08415778265983555\n",
      "Episode: 60000\n",
      "Epsilon: 0.08207986830082019\n",
      "Time Average: 0.006390575885772705\n",
      "Mean Reward: 238.114\n"
     ]
    }
   ],
   "source": [
    "#iterate through our epochs\n",
    "for epoch in range(epochs + 1): \n",
    "    #set the initial time, so we can calculate how much each action takes\n",
    "    t_initial = time.time() \n",
    "    \n",
    "    #get the discrete state for the restarted environment, so we know what's going on\n",
    "    discrete_state = get_discrete_state(env.reset()) \n",
    "    \n",
    "    #we create a boolean that will tell us whether our game is running or not\n",
    "    done = False\n",
    "    \n",
    "    #our reward is intialized at zero at the beginning of every eisode\n",
    "    epoch_reward = 0 \n",
    "\n",
    "    #Every 1000 epochs we have an episode\n",
    "    if epoch % 1000 == 0: \n",
    "        print(\"Episode: \" + str(epoch))\n",
    "\n",
    "    while not done: \n",
    "        #Now we are in our gameloop\n",
    "        #if some random number is greater than epsilon, then we take the best possible action we have explored so far\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        \n",
    "        #else, we will explore and take a random action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) \n",
    "\n",
    "        #now we will intialize our new_state, reward, and done variables\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "    \n",
    "        epoch_reward += reward \n",
    "        \n",
    "        #we discretize our new state\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        \n",
    "        #we render our environment after 2000 steps\n",
    "        if epoch % 2000 == 0: \n",
    "            env.render()\n",
    "\n",
    "        #if the game loop is still running update the q-table\n",
    "        if not done:\n",
    "            max_new_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - lr) * current_q + lr * (reward + gamma* max_new_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "    # if our epsilon is greater than .05m , and if our reward is greater than the previous and if we reached past our 10000 epoch, we recalculate episilon\n",
    "    \n",
    "    if epsilon > 0.05: \n",
    "        if epoch_reward > prev_reward and epoch > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, epoch - 10000)\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    #we calculate the final time\n",
    "    tfinal = time.time() \n",
    "    \n",
    "    #total epoch time\n",
    "    episode_total = tfinal - t_initial \n",
    "    total_time += episode_total\n",
    "    \n",
    "    #calculate and update rewards\n",
    "    total_reward += epoch_reward\n",
    "    prev_reward = epoch_reward\n",
    "\n",
    "    #every 1000 episodes print the average time and the average reward\n",
    "    if epoch % 1000 == 0: \n",
    "        mean = total_time / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total_time = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CartPole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
